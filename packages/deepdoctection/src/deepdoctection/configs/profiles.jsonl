{"name": "layout/d2_model_0829999_layout_inf_only.pt", "description": "Detectron2 layout detection model trained on Publaynet", "size": [274632215], "config": "layout/CASCADE_RCNN_R_50_FPN_GN.yaml", "preprocessor_config": null, "hf_repo_id": "deepdoctection/d2_casc_rcnn_X_32xd4_50_FPN_GN_2FC_publaynet_inference_only", "hf_model_name": "d2_model_0829999_layout_inf_only.pt", "hf_config_file": ["Base-RCNN-FPN.yaml", "CASCADE_RCNN_R_50_FPN_GN.yaml"], "urls": null, "categories": {"1": "text", "2": "title", "3": "list", "4": "table", "5": "figure"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "D2FrcnnDetector", "architecture": null, "padding": null}
{"name": "layout/d2_model_0829999_layout_inf_only.ts", "description": "Detectron2 layout detection model trained on Publaynet. Torchscript export", "size": [274974842], "config": "layout/CASCADE_RCNN_R_50_FPN_GN_TS.yaml", "preprocessor_config": null, "hf_repo_id": "deepdoctection/d2_casc_rcnn_X_32xd4_50_FPN_GN_2FC_publaynet_inference_only", "hf_model_name": "d2_model_0829999_layout_inf_only.ts", "hf_config_file": ["CASCADE_RCNN_R_50_FPN_GN_TS.yaml"], "urls": null, "categories": {"1": "text", "2": "title", "3": "list", "4": "table", "5": "figure"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "D2FrcnnTracingDetector", "architecture": null, "padding": null}
{"name": "cell/d2_model_1849999_cell_inf_only.pt", "description": "Detectron2 cell detection inference only model trained on Pubtabnet", "size": [274583063],  "config": "cell/CASCADE_RCNN_R_50_FPN_GN.yaml", "preprocessor_config": null, "hf_repo_id": "deepdoctection/d2_casc_rcnn_X_32xd4_50_FPN_GN_2FC_pubtabnet_c_inference_only", "hf_model_name": "d2_model_1849999_cell_inf_only.pt", "hf_config_file": ["Base-RCNN-FPN.yaml", "CASCADE_RCNN_R_50_FPN_GN.yaml"], "urls": null, "categories": {"1": "cell"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "D2FrcnnDetector", "architecture": null, "padding": null}
{"name": "cell/d2_model_1849999_cell_inf_only.ts", "description": "Detectron2 cell detection inference only model trained on Pubtabnet. Torchscript export", "size": [274898682],  "config": "cell/CASCADE_RCNN_R_50_FPN_GN_TS.yaml", "preprocessor_config": null, "hf_repo_id": "deepdoctection/d2_casc_rcnn_X_32xd4_50_FPN_GN_2FC_pubtabnet_c_inference_only", "hf_model_name": "d2_model_1849999_cell_inf_only.ts", "hf_config_file": ["CASCADE_RCNN_R_50_FPN_GN_TS.yaml"], "urls": null, "categories": {"1": "cell"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "D2FrcnnTracingDetector", "architecture": null, "padding": null}
{"name": "item/d2_model_1639999_item_inf_only.pt", "description": "Detectron2 item detection model inference only trained on Pubtabnet", "size": [274595351], "config": "item/CASCADE_RCNN_R_50_FPN_GN.yaml", "preprocessor_config": null, "hf_repo_id": "deepdoctection/d2_casc_rcnn_X_32xd4_50_FPN_GN_2FC_pubtabnet_rc_inference_only", "hf_model_name": "d2_model_1639999_item_inf_only.pt", "hf_config_file": ["Base-RCNN-FPN.yaml", "CASCADE_RCNN_R_50_FPN_GN.yaml"], "urls": null, "categories": {"1": "row", "2": "column"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "D2FrcnnDetector", "architecture": null, "padding": null}
{"name": "item/d2_model_1639999_item_inf_only.ts", "description": "Detectron2 cell detection inference only model trained on Pubtabnet. Torchscript export", "size": [274910970], "config": "item/CASCADE_RCNN_R_50_FPN_GN_TS.yaml", "preprocessor_config": null, "hf_repo_id": "deepdoctection/d2_casc_rcnn_X_32xd4_50_FPN_GN_2FC_pubtabnet_rc_inference_only", "hf_model_name": "d2_model_1639999_item_inf_only.ts", "hf_config_file": ["CASCADE_RCNN_R_50_FPN_GN_TS.yaml"], "urls": null, "categories": {"1": "row", "2": "column"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "D2FrcnnTracingDetector", "architecture": null, "padding": null}
{"name": "nielsr/lilt-xlm-roberta-base/pytorch_model.bin", "description": "LiLT build with a RobertaXLM base model", "size": [1136743583], "config": "nielsr/lilt-xlm-roberta-base/config.json", "preprocessor_config": null, "hf_repo_id": "nielsr/lilt-xlm-roberta-base", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "SCUT-DLVCLab/lilt-infoxlm-base/pytorch_model.bin", "description": "Language-Independent Layout Transformer - InfoXLM model by stitching a pre-trained InfoXLM and a pre-trained Language-Independent Layout Transformer (LiLT) together. It was introduced in the paper LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Wang et al. and first released in this repository.", "size": [1136743583], "config": "SCUT-DLVCLab/lilt-infoxlm-base/config.json", "preprocessor_config": null, "hf_repo_id": "SCUT-DLVCLab/lilt-infoxlm-base", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "SCUT-DLVCLab/lilt-roberta-en-base/pytorch_model.bin", "description": "Language-Independent Layout Transformer - RoBERTa model by stitching a pre-trained RoBERTa (English) and a pre-trained Language-Independent Layout Transformer (LiLT) together. It was introduced in the paper LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Wang et al. and first released in this repository.", "size": [523151519], "config": "SCUT-DLVCLab/lilt-roberta-en-base/config.json", "preprocessor_config": null, "hf_repo_id": "SCUT-DLVCLab/lilt-roberta-en-base", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "microsoft/layoutlm-base-uncased/pytorch_model.bin", "description": "LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archived the SOTA results on multiple datasets. This model does notcontain any head and has to be fine tuned on a downstream task. This is model has been trained on 11M documents for 2 epochs.  Configuration: 12-layer, 768-hidden, 12-heads, 113M parameters", "size": [453093832],  "config": "microsoft/layoutlm-base-uncased/config.json", "preprocessor_config": null, "hf_repo_id": "microsoft/layoutlm-base-uncased", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "microsoft/layoutlm-large-uncased/pytorch_model.bin", "description": "LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archived the SOTA results on multiple datasets. This model does notcontain any head and has to be fine tuned on a downstream task. This is model has been trained on 11M documents for 2 epochs.  Configuration: 24-layer, 1024-hidden, 16-heads, 343M parameters", "size": [1361845448],  "config": "microsoft/layoutlm-large-uncased/config.json", "preprocessor_config": null, "hf_repo_id": "microsoft/layoutlm-large-uncased", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "microsoft/layoutlmv2-base-uncased/pytorch_model.bin", "description": "LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. It outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including , including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.834 → 0.852), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672). The license is cc-by-nc-sa-4.0", "size": [802243295], "config": "microsoft/layoutlmv2-base-uncased/config.json", "preprocessor_config": null, "hf_repo_id": "microsoft/layoutlmv2-base-uncased", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "microsoft/layoutxlm-base/pytorch_model.bin", "description": "Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also introduce a multilingual form understanding benchmark dataset named XFUN, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled for each language. Experiment results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUN dataset. The license is cc-by-nc-sa-4.0", "size": [1476537178], "config": "microsoft/layoutxlm-base/config.json", "preprocessor_config": null, "hf_repo_id": "microsoft/layoutxlm-base", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "microsoft/layoutlmv3-base/pytorch_model.bin", "description": "LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis. The license is cc-by-nc-sa-4.0", "size": [501380823], "config": "microsoft/layoutlmv3-base/config.json", "preprocessor_config": null, "hf_repo_id": "microsoft/layoutlmv3-base", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "microsoft/table-transformer-detection/model.safetensors", "description": "Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al. This model is devoted to table detection", "size": [115393245],  "config": "microsoft/table-transformer-detection/config.json", "preprocessor_config": "microsoft/table-transformer-detection/preprocessor_config.json", "hf_repo_id": "microsoft/table-transformer-detection", "hf_model_name": "model.safetensors", "hf_config_file": ["config.json", "preprocessor_config.json"], "urls": null, "categories": {"1": "table", "2": "table_rotated"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "HFDetrDerivedDetector", "architecture": null, "padding": null}
{"name": "microsoft/table-transformer-structure-recognition/model.safetensors", "description": "Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al. This model is devoted to table structure recognition and assumes to receive a croppedtable as input. It will predict rows, column and spanning cells", "size": [115509981], "config": "microsoft/table-transformer-structure-recognition/config.json", "preprocessor_config": "microsoft/table-transformer-structure-recognition/preprocessor_config.json", "hf_repo_id": "microsoft/table-transformer-structure-recognition", "hf_model_name": "model.safetensors", "hf_config_file": ["config.json", "preprocessor_config.json"], "urls": null, "categories": {"1": "table", "2": "column", "3": "row", "4": "column_header", "5": "projected_row_header", "6": "spanning"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "HFDetrDerivedDetector", "architecture": null, "padding": null}
{"name": "doctr/db_resnet50/db_resnet50-ac60cadc.pt", "description": "Doctr implementation of DBNet from “Real-time Scene Text Detection with Differentiable Binarization”. For more information please check https://mindee.github.io/doctr/using_doctr/using_models.html#. This is the Pytorch artefact.", "size": [101971449], "config": null, "preprocessor_config": null, "hf_repo_id": null, "hf_model_name": null, "hf_config_file": null, "urls": ["https://doctr-static.mindee.com/models?id=v0.3.1/db_resnet50-ac60cadc.pt&src=0"], "categories": {"1": "word"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "DoctrTextlineDetector", "architecture": "db_resnet50", "padding": null}
{"name": "doctr/crnn_vgg16_bn/crnn_vgg16_bn-0417f351.pt", "description": "Doctr implementation of CRNN from “An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition”. For more information please check https://mindee.github.io/doctr/using_doctr/using_models.html#. This is the Pytorch artefact.", "size": [63303144], "config": null, "preprocessor_config": null, "hf_repo_id": null, "hf_model_name": null, "hf_config_file": null, "urls": ["https://doctr-static.mindee.com/models?id=v0.12.0/crnn_vgg16_bn-0417f351.pt&src=0"], "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "DoctrTextRecognizer", "architecture": "crnn_vgg16_bn", "padding": null}
{"name": "FacebookAI/xlm-roberta-base/pytorch_model.bin", "description": "XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository.", "size": [1115590446], "config": "FacebookAI/xlm-roberta-base/config.json", "preprocessor_config": null, "hf_repo_id": "FacebookAI/xlm-roberta-base", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": null, "architecture": null, "padding": null}
{"name": "deepdoctection/tatr_tab_struct_v2/pytorch_model.bin", "description": "Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paper Aligning benchmark datasets for table structure recognition by Smock et al. This model is devoted to table structure recognition and assumes to receive a slightly croppedtable as input. It will predict rows, column and spanning cells. Use a padding of around 5 pixels", "size": [115511753],  "config": "deepdoctection/tatr_tab_struct_v2/config.json", "preprocessor_config": "deepdoctection/tatr_tab_struct_v2/preprocessor_config.json", "hf_repo_id": "deepdoctection/tatr_tab_struct_v2", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json", "preprocessor_config.json"], "urls": null, "categories": {"1": "table", "2": "column", "3": "row", "4": "column_header", "5": "projected_row_header", "6": "spanning"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "HFDetrDerivedDetector", "architecture": null, "padding": null}
{"name": "Felix92/doctr-torch-parseq-multilingual-v1/pytorch_model.bin", "description": "", "size": [63286381],  "config": "Felix92/doctr-torch-parseq-multilingual-v1/config.json", "preprocessor_config": null, "hf_repo_id": "Felix92/doctr-torch-parseq-multilingual-v1", "hf_model_name": "pytorch_model.bin", "hf_config_file": ["config.json"], "urls": null, "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "DoctrTextRecognizer", "architecture": "parseq", "padding": null}
{"name": "doctr/crnn_vgg16_bn/master-fde31e4a.pt", "description": "MASTER", "size": [63286381], "config": null, "preprocessor_config": null, "hf_repo_id": null, "hf_model_name": null, "hf_config_file": null, "urls": ["https://doctr-static.mindee.com/models?id=v0.7.0/master-fde31e4a.pt&src=0"], "categories": {}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "DoctrTextRecognizer", "architecture": "master", "padding": null}
{"name": "Aryn/deformable-detr-DocLayNet/model.safetensors", "description": "Deformable DEtection TRansformer (DETR), trained on DocLayNet (including 80k annotated pages in 11 classes).", "size": [115511753], "config": "Aryn/deformable-detr-DocLayNet/config.json", "preprocessor_config": "Aryn/deformable-detr-DocLayNet/preprocessor_config.json", "hf_repo_id": "Aryn/deformable-detr-DocLayNet", "hf_model_name": "model.safetensors", "hf_config_file": ["config.json", "preprocessor_config.json"], "urls": null, "categories": {"1": "default_type", "2": "caption", "11": "text", "12": "title", "3": "footnote", "4": "formula", "5": "list_item", "6": "page_footer", "7": "page_header", "8": "figure", "9": "section_header", "10": "table"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "HFDetrDerivedDetector", "architecture": null, "padding": null}
{"name": "deepdoctection/tatr_tab_struct_v2/model.safetensors", "description": "Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paper Aligning benchmark datasets for table structure recognition by Smock et al. This model is devoted to table structure recognition and assumes to receive a slightly croppedtable as input. It will predict rows, column and spanning cells. Use a padding of around 5 pixels. This artefact has been converted from deepdoctection/tatr_tab_struct_v2/pytorch_model.bin and should be used to reduce security issues", "size": [115511753], "config": "deepdoctection/tatr_tab_struct_v2/config.json", "preprocessor_config": "deepdoctection/tatr_tab_struct_v2/preprocessor_config.json", "hf_repo_id": "deepdoctection/tatr_tab_struct_v2", "hf_model_name": "model.safetensors", "hf_config_file": ["config.json", "preprocessor_config.json"], "urls": null, "categories": {"1": "table", "2": "column", "3": "row", "4": "column_header", "5": "projected_row_header", "6": "spanning"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "HFDetrDerivedDetector", "architecture": null, "padding": null}
{"name": "papluca/xlm-roberta-base-language-detection/model.safetensors", "description": "This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.", "size": [101971449], "config": "papluca/xlm-roberta-base-language-detection/config.json", "preprocessor_config": null, "hf_repo_id": "papluca/xlm-roberta-base-language-detection", "hf_model_name": "model.safetensors", "hf_config_file": ["config.json", "sentencepiece.bpe.model","special_tokens_map.json","tokenizer.json","tokenizer_config.json"], "urls": null, "categories": {"1": "jpn", "2": "dut", "3": "ara", "4": "pol", "5": "deu", "6": "ita", "7": "por", "8": "tur", "9": "spa", "10": "hin", "11": "gre", "12": "urd", "13": "bul", "14": "eng", "15": "fre", "16": "chi", "17": "rus", "18": "tha", "19": "swa", "20": "vie"}, "categories_orig": null, "dl_library": "PT", "model_wrapper": "HFLmLanguageDetector", "architecture": null, "padding": null}